{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352744\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "dataset=pd.read_csv(r'C:\\Users\\HP\\Desktop\\ML\\Reviews.csv')\n",
    "dataset=dataset.drop_duplicates(subset={'UserId','Time'},keep='first',inplace=False)\n",
    "#dataset=dataset.sample(10000)\n",
    "#dataset.reset_index(inplace = True) \n",
    "print(len(dataset))\n",
    "dataset=dataset.sort_values('Time',ascending=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought these for the office and no one can put them down. I'm ordering more today and it has only been 2 weeks. Didn't have to shop for them either, Amazon brought it to my door!\n",
      "(352744, 10)\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Text'][500])\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 352744/352744 [08:02<00:00, 731.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sir kensington great job updat classic ketchup wonder product refresh tast ketchup great updat leav disappoint given heinz restaur heinz die hard fan ketchup wish alway knew ketchup could without chemic aftertast found heinz sure give ketchup tri forget spice varieti purchas pack classic spice varieti first sir kensington experi definit way'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "  \n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "sno=SnowballStemmer('english')\n",
    "\n",
    "def clean_html(sentence):\n",
    "    x=re.compile('<.*?>')\n",
    "    cleantext=re.sub(x,' ',sentence)\n",
    "    return cleantext\n",
    "\n",
    "def clean_punc(sentence):\n",
    "    cleaned=re.sub(r'[?|!|\\'|\"|#|.|,|)|(|\\|/]',r' ',sentence)\n",
    "    return cleaned\n",
    "\n",
    "p_text=[] #processed text\n",
    "from tqdm import tqdm\n",
    "for sent in tqdm(dataset['Text'].values,position=0,leave=True):\n",
    "    sent=clean_html(sent)\n",
    "    sent=clean_punc(sent)\n",
    "    s=[]\n",
    "    for w in sent.split():\n",
    "        if(w.isalpha() and len(w)>2 and w.lower() not in stop):\n",
    "            w=(sno.stem(w.lower()))\n",
    "            #w=lemmatizer.lemmatize(w.lower()) stemming was producing better results\n",
    "            s.append(w)\n",
    "    sent=\" \".join(s)\n",
    "    p_text.append(sent)\n",
    "    \n",
    "dataset['p_text']=p_text\n",
    "\n",
    "dataset['p_text'][377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered these for my coffee themed wedding. When they arrived I had to fight off friends because they smelled and tasted so good. I literally had to hide the box to the wedding! They were a big hit on my wedding day and there were none left over that we put on the tables. It was a great tasting product for the lowest price I've seen!\n",
      "order coffe theme wed arriv fight friend smell tast good liter hide box wed big hit wed day none left put tabl great tast product lowest price seen\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Text'][347])\n",
    "print(dataset['p_text'][347])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 352744/352744 [00:03<00:00, 90395.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21649"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences=[]\n",
    "for sent in tqdm(p_text,position=0,leave=True):\n",
    "    sentences.append(sent.split())\n",
    "\n",
    "model=Word2Vec(sentences,min_count=5,size=100,workers=16)\n",
    "w2v_words=dict(model.wv.vocab)\n",
    "len(w2v_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 352744/352744 [01:53<00:00, 3101.78it/s]\n"
     ]
    }
   ],
   "source": [
    "text_vectors=[]\n",
    "            \n",
    "for sent in tqdm(sentences,position=0,leave=True):\n",
    "    text_vec=np.zeros(100)\n",
    "    word_cnt=0\n",
    "    for word in sent:\n",
    "        if word in w2v_words:\n",
    "            text_vec+=model.wv[word]\n",
    "            word_cnt+=1\n",
    "            \n",
    "    if word_cnt!=0:\n",
    "        text_vec/=word_cnt\n",
    "    text_vectors.append(text_vec)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.14442891e-01, -1.42210147e-01, -3.26645710e-01, -2.44100249e-01,\n",
       "        3.66878754e-01, -6.75766521e-01,  3.32401447e-01, -3.04388396e-03,\n",
       "        9.10550290e-02,  3.52286058e-02,  3.72974381e-01,  4.74310299e-01,\n",
       "       -6.15902463e-01,  1.81039186e-01,  2.08561194e-01,  2.11502080e-01,\n",
       "       -9.25837468e-02,  1.83417681e-01, -3.36821617e-01,  2.03735961e-01,\n",
       "       -9.08740363e-02, -4.06576183e-02,  1.63127245e-01,  1.66352998e-01,\n",
       "        1.72943867e-02,  4.88694422e-02, -1.75357166e-01,  3.81489201e-02,\n",
       "        3.67015706e-01,  2.04396301e-01, -5.24735602e-01,  6.44016188e-02,\n",
       "       -1.06286212e-01, -7.72363675e-03,  2.62010783e-02,  4.34278399e-01,\n",
       "       -2.36766308e-01, -4.62146314e-01,  1.90988092e-01,  1.75732060e-01,\n",
       "        1.64253786e-01,  3.09478768e-01,  3.00733639e-01, -7.61912188e-01,\n",
       "       -5.70509087e-01, -3.44225158e-01, -3.21389690e-01, -3.40706823e-01,\n",
       "       -6.77278182e-02, -5.90705422e-03, -1.82634458e-01,  4.28602356e-02,\n",
       "        6.83472390e-01, -9.16242128e-03, -3.73703443e-01,  1.04640556e-04,\n",
       "       -9.45340661e-02, -2.01481588e-01, -2.72160707e-01,  1.13148883e-01,\n",
       "       -5.08242109e-04,  3.25599246e-01, -4.31434237e-01, -4.05814612e-01,\n",
       "       -1.76264912e-01,  2.61780315e-01, -6.88787115e-01, -4.71486846e-02,\n",
       "        1.65765008e-01,  2.57684937e-01,  1.24517662e-01,  1.09177707e-01,\n",
       "        4.21337830e-01,  2.80519348e-02,  1.55072583e-01,  7.31508295e-03,\n",
       "        2.28341742e-02, -1.87469201e-01,  5.02649013e-01, -2.71915115e-01,\n",
       "       -4.76051035e-02,  7.32954648e-02, -8.39646609e-02, -3.28679527e-02,\n",
       "        1.96061017e-01, -4.98889481e-02, -4.68860616e-01, -2.75524917e-01,\n",
       "       -1.72695715e-01,  2.32848441e-01,  4.16992740e-01, -7.30067327e-01,\n",
       "        4.45032922e-01,  1.03382395e-01, -1.70978794e-01, -4.62590933e-01,\n",
       "       -1.42661267e-01, -2.77124194e-01,  1.77385233e-01,  5.42923877e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    224921\n",
       "4     48949\n",
       "1     33856\n",
       "3     26187\n",
       "2     18831\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352744\n",
      "273870\n"
     ]
    }
   ],
   "source": [
    "X=text_vectors\n",
    "Y=[]\n",
    "p_reviews=0\n",
    "n_reviews=0\n",
    "for score in dataset['Score'].values:\n",
    "    if score >=4:\n",
    "        Y.append(1)\n",
    "        p_reviews+=1\n",
    "    else:\n",
    "        Y.append(0)\n",
    "        n_reviews+=1\n",
    "        \n",
    "dataset['class']=Y\n",
    "print(len(Y))\n",
    "print(p_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=25, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh=KNeighborsClassifier(n_neighbors=25)\n",
    "neigh.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "\n",
    "#x_train=X[0:7000]\n",
    "#x_test=X[7000:] #70:30\n",
    "#y_train=Y[0:7000]\n",
    "#y_test=Y[7000:] #70:30\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=30)\n",
    "optimal_k=1\n",
    "a_max=0\n",
    "print(x_train)\n",
    "for k in tqdm(range(1,40,2),position=0,leave=True):\n",
    "    knn=KNeighborsClassifier(n_neighbors=k)\n",
    "    scores=cross_val_score(knn,x_train,y_train,cv=10,scoring='accuracy',n_jobs=-1)\n",
    "    if scores.mean()>a_max:\n",
    "        a_max=scores.mean()\n",
    "        optimal_k=k\n",
    "        \n",
    "print(optimal_k)\n",
    "knn=KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn.fit(x_train,y_train)\n",
    "pred=knn.predict(x_test)\n",
    "\n",
    "test_acc=accuracy_score(y_test,pred)*100\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'product', 'better']\n",
      "Positive Review with 0.88 probabilty\n"
     ]
    }
   ],
   "source": [
    "r=\"good product but can be better\"\n",
    "r=clean_html(r)\n",
    "r=clean_punc(r)\n",
    "test_review=[]\n",
    "for w in r.split():\n",
    "    if(w.isalpha() and len(w)>2 and w.lower() not in stop):\n",
    "        w=(sno.stem(w.lower()))\n",
    "        test_review.append(w)\n",
    "        \n",
    "\n",
    "vec=np.zeros(100)\n",
    "cnt_words=0\n",
    "\n",
    "for word in test_review:\n",
    "    if word in w2v_words:\n",
    "        vec+=model.wv[word]\n",
    "        cnt_words+=1\n",
    "        \n",
    "if cnt_words!=0:\n",
    "    vec/=cnt_words\n",
    "    \n",
    "print(test_review)\n",
    "        \n",
    "probs=neigh.predict_proba([vec])[0]\n",
    "ans=\"\"\n",
    "if neigh.predict([vec])==1:\n",
    "    ans+=\"Positive Review with \"+str(probs[1])+\" probabilty\"\n",
    "else:\n",
    "    ans+=\"Negative Review with \"+str(probs[0])+\" probability\"\n",
    "\n",
    "print(ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon-review-analysis-knn.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(neigh, 'amazon-review-analysis-knn.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon-review-analysis-w2v.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'amazon-review-analysis-w2v.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
